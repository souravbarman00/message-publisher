# ğŸš€ Message Publisher System - Complete Setup Guide

A production-ready full-stack application for publishing messages to **Kafka**, **SNS**, and **SQS** with separate API and worker services, following your existing project structure.

## ğŸ“‹ Project Overview

Your message publisher system is now organized into separate, scalable services:

```
ğŸ“¦ message-publisher/
â”œâ”€â”€ ğŸŒ api/                    # Express.js API service (Port 4000)
â”œâ”€â”€ âš™ï¸  workers/               # Message processing workers  
â”œâ”€â”€ ğŸ¨ frontend/               # React frontend (Port 3000)
â”œâ”€â”€ ğŸ”§ shared/                 # Shared utilities and constants
â”œâ”€â”€ ğŸ“œ scripts/                # Setup and initialization scripts
â”œâ”€â”€ ğŸ³ docker-compose.yml      # Development environment
â”œâ”€â”€ âš¡ setup.sh/.bat           # Quick setup scripts
â””â”€â”€ ğŸ“– README.md               # This file
```

## ğŸ¯ Features

### âœ¨ Frontend (React)
- ğŸ¨ Modern UI with Tailwind CSS and Lucide icons
- ğŸ”„ Real-time message publishing with multiple service options
- ğŸ“Š Message history and status tracking
- ğŸ¥ Service health monitoring
- ğŸ“± Responsive design for all devices

### ğŸš€ API Service (Express.js)
- âš¡ High-performance REST API with ES modules
- ğŸ¯ Five publishing endpoints:
  - `POST /api/publisher/kafka-sns` - Dual publishing
  - `POST /api/publisher/sns-sqs` - SNS to SQS flow
  - `POST /api/publisher/kafka` - Kafka only
  - `POST /api/publisher/sns` - SNS only
  - `POST /api/publisher/sqs` - SQS only
- ğŸ›¡ï¸ Input validation and error handling
- ğŸ“Š Health checks and service monitoring
- ğŸ”„ Concurrent message processing

### âš™ï¸ Worker Services
- **Kafka Worker**: Real-time message consumption with batching
- **SQS Worker**: Long-polling with automatic message deletion
- **SNS Worker**: Delivery monitoring and notification processing
- ğŸ”„ Automatic reconnection and graceful shutdown
- ğŸ“Š Processing statistics and health monitoring

## ğŸš€ Quick Start

### Prerequisites
- Node.js 18+ âœ…
- Running Kafka instance (localhost:9092) âœ…
- AWS credentials or LocalStack for development âœ…

### 1. One-Command Setup
```bash
# Linux/macOS
./setup.sh setup

# Windows
setup.bat setup
```

### 2. Configure Environment
Update the `.env` files created in `api/` and `workers/` directories:

```env
# AWS Configuration
AWS_REGION=us-east-1
AWS_ACCESS_KEY_ID=your_access_key_here
AWS_SECRET_ACCESS_KEY=your_secret_key_here

# SNS Configuration
SNS_TOPIC_ARN=arn:aws:sns:us-east-1:123456789012:your-topic-name

# SQS Configuration
SQS_QUEUE_URL=https://sqs.us-east-1.amazonaws.com/123456789012/your-queue-name

# Kafka Configuration
KAFKA_BROKERS=localhost:9092
KAFKA_TOPIC=messages
```

### 3. Start All Services
```bash
# One command to start everything
./setup.sh start

# Or individually:
./setup.sh start-api      # API only
./setup.sh start-workers  # Workers only  
./setup.sh start-frontend # Frontend only
```

### 4. Access Your Application
- ğŸŒ **Frontend**: http://localhost:3000
- ğŸ”§ **API**: http://localhost:4000
- ğŸ“š **API Docs**: http://localhost:4000/api/docs
- â¤ï¸ **Health Check**: http://localhost:4000/api/health

## ğŸ³ Development with Docker

For a complete development environment with Kafka, LocalStack (AWS), and databases:

```bash
# Start infrastructure
docker-compose up -d

# Initialize AWS services in LocalStack
chmod +x scripts/init-localstack.sh
./scripts/init-localstack.sh

# Start your services
./setup.sh start
```

This gives you:
- ğŸ“Š **Kafka + Kafka UI**: http://localhost:8080
- â˜ï¸ **LocalStack (AWS)**: http://localhost:4566
- ğŸ—„ï¸ **PostgreSQL**: localhost:5432
- ğŸ”„ **Redis**: localhost:6379

## ğŸ”§ API Usage Examples

### Send Message to Kafka + SNS
```bash
curl -X POST http://localhost:4000/api/publisher/kafka-sns \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Hello from the message publisher!",
    "metadata": {
      "priority": "high",
      "source": "api-test"
    }
  }'
```

### Response Format
```json
{
  "success": true,
  "requestId": "req_1234567890_abc123def",
  "message": "Message processing completed",
  "timestamp": "2024-01-01T00:00:00.000Z",
  "results": {
    "kafka": {
      "status": "success",
      "topic": "messages",
      "partition": 0,
      "offset": "12345"
    },
    "sns": {
      "status": "success", 
      "messageId": "sns-msg-id-12345"
    }
  }
}
```

## ğŸ“Š Architecture Integration

Your new system integrates seamlessly with your existing project structure:

```
ğŸ“ C:\Users\saura\Documents\project\
â”œâ”€â”€ ğŸ“Š kafka-demo/              # Your existing Kafka setup
â”œâ”€â”€ ğŸ”” sns-test/                # Your existing SNS setup  
â”œâ”€â”€ ğŸ”„ sqsWorker.js             # Your existing SQS worker
â”œâ”€â”€ ğŸŒ routes/sqs.js            # Your existing SQS routes
â””â”€â”€ ğŸ†• message-publisher/       # New integrated system
    â”œâ”€â”€ api/                    # Replaces server.js functionality
    â”œâ”€â”€ workers/                # Enhances your sqsWorker.js
    â””â”€â”€ frontend/               # New React interface
```

## ğŸ”„ Message Flow

1. **Frontend** â†’ User selects publishing method and sends message
2. **API Service** â†’ Validates input and publishes to selected services
3. **Message Services** â†’ Kafka/SNS/SQS receive and queue messages
4. **Workers** â†’ Process messages asynchronously with error handling
5. **Frontend** â†’ Shows real-time status updates and history

## ğŸ› ï¸ Development Commands

```bash
# Install all dependencies
npm run install:all

# Start individual services
npm run dev:api       # API server
npm run dev:workers   # All workers  
npm run dev:frontend  # React app

# Start everything
npm run dev:all

# Build frontend for production
npm run build:frontend
```

## ğŸ“± Production Deployment

### Environment Variables
Set these in your production environment:
- `NODE_ENV=production`
- AWS credentials via IAM roles (recommended)
- Kafka cluster endpoints
- SNS topic ARNs and SQS queue URLs
- Database connection strings (if using tracking)

### Docker Deployment
Each service can be containerized independently:

```dockerfile
# Example: API service
FROM node:18-alpine
WORKDIR /app
COPY api/ .
RUN npm ci --only=production
EXPOSE 4000
CMD ["npm", "start"]
```

## ğŸ› Troubleshooting

### Common Issues

**ğŸš« Kafka Connection Failed**
```bash
# Check if Kafka is running
nc -z localhost 9092

# Start Kafka (if using Docker)
docker-compose up -d kafka
```

**âŒ AWS Service Errors**
```bash
# Test AWS connectivity
aws sts get-caller-identity

# Use LocalStack for development
./scripts/init-localstack.sh
```

**âš ï¸ Port Already in Use**
```bash
# Check what's using the port
lsof -ti:4000
lsof -ti:3000

# Kill process if needed
kill -9 $(lsof -ti:4000)
```

## ğŸ‰ What's New in Your System

Compared to your existing setup, this adds:

### âœ… Enhanced Features
- **Unified Frontend**: Single interface for all publishing methods
- **Concurrent Processing**: Publish to multiple services simultaneously  
- **Better Error Handling**: Partial success handling and retry logic
- **Real-time Monitoring**: Live status updates and service health checks
- **Production Ready**: Proper logging, validation, and graceful shutdown

### ğŸ”§ Improved Architecture
- **Separation of Concerns**: API, Workers, and Frontend are independent
- **Scalability**: Each service can be scaled independently
- **Maintainability**: Shared utilities and consistent structure
- **Development Experience**: Hot reloading, easy setup, and comprehensive docs

## ğŸ¤ Integration with Your Existing Code

Your existing files remain functional and can be gradually migrated:
- `server.js` functionality is enhanced in `api/server.js`
- `sqsWorker.js` is evolved into `workers/sqs-worker.js`
- `routes/sqs.js` patterns are expanded in `api/routes/publisher.js`
- Your Kafka and SNS demos provide the foundation for the new services

## ğŸ“ Support

If you encounter any issues:

1. **Check the logs**: Each service provides detailed logging
2. **Run health checks**: Visit http://localhost:4000/api/health
3. **Verify configuration**: Ensure all `.env` files are properly configured
4. **Test individually**: Start one service at a time to isolate issues

## ğŸ¯ Next Steps

1. **Configure your AWS credentials** in the `.env` files
2. **Start your existing Kafka instance** (or use Docker)
3. **Run the setup script**: `./setup.sh setup`
4. **Launch the system**: `./setup.sh start`
5. **Open the frontend**: http://localhost:3000
6. **Send your first message**! ğŸš€

---

**ğŸ‰ Congratulations!** Your message publisher system is now ready for development and production use!
